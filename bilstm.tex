\documentclass[9pt]{beamer}
\usetheme{Madrid}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{adjustbox}

% PRESENTER NOTES
\usepackage{pgfpages}
\setbeamertemplate{note page}[plain]
\setbeameroption{show notes on second screen=right}

\usepackage{array}  % image alignment in tabular
\author{Pietro Mascolo}
\title{Sequence modeling for product classification}
\logo{\includegraphics[height=0.45cm]{flow_resources/flow-logo-icon-&-type-color.png}}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
\institute{\href{http://www.flow.io}{Flow Comerce}} 
%\date{} 
%\subject{} 



% Flow palette
\definecolor{FlowBlue}{RGB}{00,81,178}
\definecolor{VibrantBlue}{RGB}{71,145,255}
\definecolor{Steel}{RGB}{128,128,128}
\definecolor{Silver}{RGB}{191,191,191}
\definecolor{LightSilver}{RGB}{220,220,220}
\definecolor{VeryLightSilver}{RGB}{240,240,240}
\definecolor{Accent1}{RGB}{85,218,255}
\definecolor{Accent2}{RGB}{249,119,98}
\definecolor{Accent3}{RGB}{250,221,5}
\definecolor{Accent4}{RGB}{82,212,253}
\definecolor{AccentAlert}{RGB}{249,50,50}

% More info on sections and parts of beamer here: http://www.cpt.univ-mrs.fr/~masson/latex/Beamer-appearance-cheat-sheet.pdf

% general
\setbeamercolor{palette primary}{bg=FlowBlue}
\setbeamercolor{palette secondary}{bg=VibrantBlue}
\setbeamercolor{palette tertiary}{bg=Steel}
\setbeamercolor{palette quaternary}{bg=Accent1}
\setbeamercolor{palette sidebar primary}{bg=Accent1}
\setbeamercolor{palette sidebar secondary}{bg=Accent2}
\setbeamercolor{palette sidebar tertiary}{bg=Accent2}
\setbeamercolor{palette sidebar quaternary}{bg=Accent4}

% hyperlinks
\setbeamercolor{button}{use=Accent4}
\setbeamercolor{button border}{use=Accent4}

% structure
\setbeamercolor{structure}{fg=FlowBlue}
\setbeamercolor{local structure}{fg=Accent4}
\setbeamercolor{alerted text}{fg=AccentAlert, bg=LightSilver}

% block
\setbeamercolor{block title}{bg=FlowBlue}
\setbeamercolor{block body}{bg=VeryLightSilver}

% titles
\setbeamercolor{title}{bg=FlowBlue, fg=white}
\setbeamercolor{frametitle}{bg=FlowBlue, fg=white}

%itemization
%\setbeamercolor{itemize item}{bg=FlowBlue}
%\setbeamercolor{itemize subitem}{bg=FlowBlue}
%\setbeamercolor{itemize subsubitem}{bg=FlowBlue}
%\setbeamercolor{enumerate item}{bg=FlowBlue}
%\setbeamercolor{enumerate subitem}{bg=FlowBlue}
%\setbeamercolor{enumerate subsubitem}{bg=FlowBlue}
%\setbeamertemplate{itemize item}{\color{FlowBlue}}
%\setbeamertemplate{enumerate item}{\color{FlowBlue}}

%\setbeamerfont{section number projected}{family=\rmfamily,series=\bfseries,size=\normalsize}
\setbeamercolor{section number projected}{bg=FlowBlue,fg=white}
\setbeamercolor{subsection number projected}{bg=FlowBlue}

\setbeamercolor{structure}{bg=FlowBlue}

% margins
\setbeamersize{text margin left=10mm,text margin right=10mm}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\titlepage
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{This presentation}
	\tableofcontents
\end{frame}


\section{Problem statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Introduction}
	\note{We are trying to achieve classification and data enrichment of products. We are interested in knowing what these products are in order to calculate taxes and duty rates for international shipping.}
	\begin{block}{Problem statement}
		We want to classify a generic product into an HS6 code and a brief description in order to determine duty rates for a given origin and destination.
	\end{block}
	\vspace{0.5cm}
	
	\pause
	\begin{center}
		\begin{tabular}{ c  c m{5cm} }
			\begin{minipage}{4cm}
			\begin{figure}
					\includegraphics[width=4cm]{imgs/boot.png}
					 \tiny{A classically styled pair of leather Chelsea boots, featuring elasticated side panels, heel pull tabs and finished with a subtle...}
				\end{figure}
			\end{minipage} \pause
			& $\longrightarrow$ 
			& $$\left\{ 
				\begin{array}{l} 
					\text{description: womens leather boot} \\
					\\
					\text{HS6: 640391}
				\end{array} 
			\right.$$ \\
		\end{tabular}
	\end{center}
	
	\note{\\In particular we want to go from a generic description of the product to a more objective 'customs description' and to HS6 code.}
\end{frame}


\section{First approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Our initial approach}
	\begin{center}
		\includegraphics[width=0.4\textwidth,height=!,trim={0 0 30cm 5cm},clip]{imgs/houses.jpg}
	\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Initial models}
	\begin{itemize}
		\item TFIDF encoded representation of text;
		\item one model per organization (20-30);
		\item reasonably accurate.
	\end{itemize}
	\pause
	\vspace{1cm}
	\begin{center}
		\Large{\textbf{BUT...}}
	\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Little to no generalization}
	\begin{center}
		\begin{figure}
			\includegraphics[width=0.6\textwidth,height=!]{imgs/huffedandpuffed.png}
			\caption{\textit{...and he huffed and he puffed and he blew the house in...}}
		\end{figure}
	\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Problems!!}
	\begin{itemize}
		\item TFIDF encoding doesn't capture sequence relationship well! \note{TFIDF can represent some n-gram features, but in a very limited way.}
		\item Many models to keep track of!
		\item Hard performance evaluation!
		\item Some organizations have very little data to work on!  \note{\\Some orgs only have a handful of products or codes (sometimes less than 5!).}
		\item Some organizations have a very limited catalogue (1 or 2 types of items)!
	\end{itemize}

	\pause
	\begin{center}
		\includegraphics[width=0.2\textwidth]{imgs/sad.png}
	\end{center}
\end{frame}


\section{A new approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{A new approach}
	\begin{center}
		\includegraphics[width=0.4\textwidth,height=!,trim={16cm 0 17.5cm 5cm},clip]{imgs/houses.jpg}
	\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sequence modeling}
	We want to:
	\begin{itemize}
		\item Have a single general model;
		\item capture information regarding patterns in sequences of words;
		\item be able add a new organization seamlessly (if we have seen the kind of products they sell).
	\end{itemize}
	\vspace{0.5cm}
	These requirements suggest that we need a more sophisticated method than what we are using.
	
	\pause
	\vspace{1cm}
	\begin{center}
		\Large{\textbf{Enter Neural Networks...}}
	\end{center}
	
\end{frame}

\subsection{Neural networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{A primer on neural networks}
	\begin{center}
		\only<1>{
			\begin{figure}
				\includegraphics[width=\textwidth]{imgs/nn.png}
				\caption{Shallow and deep neural networks}
			\end{figure}
		}
		\only<2>{
			\begin{figure}
				\includegraphics[width=0.7\textwidth]{imgs/ann.png}
				\caption{Neuron connections}
			\end{figure}
		}
		\only<3>{
			\begin{figure}
				\includegraphics[width=\textwidth,trim={0 1cm 0 0},clip]{imgs/neuron.png}
				\caption{How a neuron works}
			\end{figure}
			\note{The activation is usually used to introduce a non-linearity. This helps these algorithms to learn complex patterns.}
		}
	\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{How Neural Networks learn - backpropagation}
	\begin{center}
		\only<1>{
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{imgs/forward-propagation.png}
				\caption{Forward propagation - getting the outputs}
			\end{figure}
			\note{Forward pass calculates output based on the current model settings. The reults are compared with real labels in order to calculate how wrong we are.}
		}
		\only<2>{
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{imgs/backpropagation.png}
				\caption{Back propagation - propagating the errors by calculating A LOT of derivatives}
			\end{figure}
			\note{backpropagation tells us the dependencies on the final output on all the intermediate model statuses. These are updated to minimize the final error.}
		}
	\end{center}
\end{frame}


\subsection{Recurrent networks and sequence modeling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sequence learning - Recurrent Neural Networks}
	\note{
		RNN cells feed themselves a portion of knowledge. This can capture dequence dependencies.\\
		This is achieved by means of an internal cell state that is fed as input to successive elements in the sequence.
	}
	\begin{center}
		\only<1>{
			\begin{figure}
				\includegraphics[width=0.3\textwidth, trim={0 0 38cm 0},clip]{imgs/recurrent.png}
				\caption{Recurrent cell}
			\end{figure}
		}
		\only<2>{
			\begin{figure}
				\includegraphics[width=\textwidth]{imgs/recurrent.png}
				\caption{Recurrent cell unfolded}
			\end{figure}
		}
	\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Short Term memory problem - vanishing or exploding gradient}
	\begin{figure}
		\includegraphics[width=\textwidth,trim={0 0 0 3cm},clip]{imgs/vanishing.png}
		\caption{Vanishing gradient illustration}
	\end{figure}
	
	\note{Dependencies 'fade' with time. Effect of derivation.}
	
	\pause
	\begin{alertblock}{Problem}
		RNN don't work well with long sequences: they lose knowledge of long term dependencies.\\
		They have a \textbf{short memory}.
	\end{alertblock}
	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Solving the short memory}
	
	\only<1>{
	
		\begin{center}
			\begin{tabular}{ c  m{5cm} }
				\begin{minipage}{0.5\textwidth}
				\begin{figure}
						\includegraphics[width=0.8\textwidth]{imgs/rnncell.png}
						 \caption{Simple RNN cell}
					\end{figure}
				\end{minipage}

				& $$
					\begin{array}{l}
						
						h_t = \tanh \left( x_{t-1} W_{hh}  +  h_{t-1} W_{xt} \right) \\
						c_t  = W_{hy} h_t
					\end{array} 
				$$ \\
			\end{tabular}
		\end{center}
	}
	
	\only<2-3>{
		\begin{block}{LSTM and GRU solve the short memory problem}{
			LSTM $\rightarrow$ Long Short Memory \\
			GRU $\rightarrow$ Gated Recurrent Unit
		}
		\end{block}
	}
	
	
	\only<2>{
	
		\begin{center}
			\begin{tabular}{ c  m{5cm} }
				\begin{minipage}{0.5\textwidth}
				\begin{figure}
						\includegraphics[width=0.8\textwidth,trim={2.6cm 0 20cm 0},clip]{imgs/lstmandgru.png}
						 \caption{LSTM cell}
					\end{figure}
				\end{minipage}

				& $$
					\begin{array}{l} 
						i_t = \sigma\left( x_t U^i + h_{t-1} W^i\right) \\
						f_t = \sigma\left( x_t U^f + h_{t-1} W^f\right) \\
						o_t = \sigma\left( x_t U^o + h_{t-1} W^o\right) \\
						\widetilde{C}_t = \tanh \left( x_t U^g + h_{t-1} W^g \right)\\
						C_t = \sigma\left( f_t \ast C_{t-1} + i_t \ast \widetilde{C}_t\right) \\
						h_t = \tanh \left( C_t \ast o_t \right)
					\end{array} 
				$$ \\
			\end{tabular}
		\end{center}
		
		\note{
			Base idea is the $C_t$, representing the cell state. It goes through the whole process.\\
			First step is deciding what information to throw away. Sigmoid 'forget gate' does this.\\
			Next step is deciding what new information we want to add to the cell state. This is done by the update gate.\\
			Update gate consists of sigmoid 'input layer gate' that decides which values we'll update.The tanh generates a vector of candidates that could be added to the state. The combination of these two updates the whole cell state.\\
			Then we update the output. We use a tanh on cell state to push values in  (-1; 1) and then we multiply by the output of output gate (sigmoid) so that we only output what is decided by the cell state.
		}
	}
	
	\only<3>{
		\begin{center}
			\begin{tabular}{ c  m{5cm} }
				\begin{minipage}{0.5\textwidth}
				\begin{figure}
						\includegraphics[width=0.8\textwidth,trim={20cm 0 3cm 0},clip]{imgs/lstmandgru.png}
						 \caption{GRU cell}
					\end{figure}
				\end{minipage}

				& $$
					\begin{array}{l} 
						z_t = \sigma\left( x_t U^z + h_{t-1} W^z\right) \\
						r_t = \sigma\left( x_t U^r + h_{t-1} W^r\right) \\
						o_t = \sigma\left( x_t U^o + h_{t-1} W^o\right) \\
						\widetilde{h}_t = \tanh \left( x_t U^h +(r_t \ast h_{t-1}) W^h \right)\\
						h_t =  \left( 1-z_t \right) \ast h_{t-1} + z_t \ast \widetilde{h}_t
					\end{array} 
				$$ \\
			\end{tabular}
		\end{center}
		\note{GRUs are a variant of LSTM cells. Forget and input gates are merged into a single Update gate that decides what to forget and what to update.\\
			It also merges the hiddent state and cell state.\\ Due to these changes, GRUs are simpler than LSTM while being almost equally powerful, and faster to train.}
	}
	
	
	%\begin{figure}
%		\includegraphics[width=\textwidth]{imgs/lstmandgru.png}
%		\caption{LSTM and GRU cell structure}
%	\end{figure}
\end{frame}


\section{Our classification model}



\subsection{Data preparation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The data}
	\begin{itemize}
		\item 1.8M products;
		\item 15K customs description;
		\item 500+ HS6 codes.
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Preparation}
	In order to use the data, we have to:
	\begin{itemize}
		\item cleant the text (remove/fix typos, remove non-meaningful tokens, etc.);
		\item remove stop words (a, the, of, ...);
		\item tokenize the words (turning text into numbers);
		\item retain only non-rare tokens.
	\end{itemize}
\end{frame}


\subsection{The model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The model}
	\begin{figure}
		\includegraphics[height=0.6\textwidth]{imgs/model.png}
		\caption{Current model architecture}
	\end{figure}
\end{frame}


\subsection{Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Results and evaluation}
\begin{figure}
		\includegraphics[width=0.7\textwidth]{imgs/training.png}
		\caption{Training and validation}
	\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Probability distribution per class on test set}
\begin{figure}
		\includegraphics[width=\textwidth]{imgs/performance.png}
		\caption{Cumulative probability distribution per class}
	\end{figure}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What about older models?}
\begin{figure}
		\includegraphics[width=0.8\textwidth, trim={0 4cm 0 2cm}, clip]{imgs/comparison.png}
		\caption{Performance compared with older Lambda models}
	\end{figure}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Deployment}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Pros and cons}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Problems??}
	\begin{center}
		\begin{figure}
			\includegraphics[width=0.6\textwidth,height=!]{imgs/huffedandpuffed.png}
			\caption{\textit{...and he huffed and he puffed, \textbf{but he hasn't blown the house in... yet...}}}
		\end{figure}
	\end{center}
\end{frame}





\section{Demo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Let's see it in action}

\begin{center}
	\Huge{\href{http://localhost:8888}{Demo!}}
\end{center}

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conclusions and future work}
	\begin{center}
		\includegraphics[width=0.4\textwidth,height=!,trim={29cm 0 0 5.2cm},clip]{imgs/houses.jpg}
	\end{center}
\end{frame}
\end{document}